{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f8a48b-0b23-4761-812a-605306ffe596",
   "metadata": {},
   "source": [
    "# code for learning lol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5cc76c-9f9e-4c38-85b7-9523ebbc4b5d",
   "metadata": {},
   "source": [
    "##### as always the chatbot will work by 1st: doing an NLP stuff, 2nd: doing some Machine Learning stuff..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c52cd3-d7cd-40b2-8f1a-a38572450dd4",
   "metadata": {},
   "source": [
    "##### the nlp will be done by\n",
    "##### 1- tokenizing\n",
    "##### 2- stemming/lemmatizing and lowering\n",
    "##### 3- excluding punctioation characters\n",
    "##### 4- throw the result in a bag of word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df4e9b2-872a-4a71-91a0-e42b58fe4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e8d71d-9696-4020-b66e-9eb092fb5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = json.loads(open('intents.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8336502-91de-44a6-aa49-92a4dde48cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bag of words is a bunch of sentances turned into 0s and 1s so we can determine how many times it has occured\n",
    "# example\n",
    "# the tag here is greeting\n",
    "# ['hello', 'hey', 'how', 'are', 'you']\n",
    "# [1, 1, 0, 0, 0]\n",
    "# ['are', 'you', 'a', 'robot', 'hello']\n",
    "# [0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f33b80e-83c4-46e0-b202-2c848f6f68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "stm = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b37879-b574-4851-a47a-a613f5b176b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentance):\n",
    "    return nltk.word_tokenize(sentance)\n",
    "\n",
    "def lemm(word):\n",
    "    return lem.lemmatize(word.lower())\n",
    "\n",
    "def stem(word):\n",
    "    return stm.stem(word.lower())\n",
    "\n",
    "# this is probably the most important function, try to understand what it does\n",
    "def bag_of_words(tokenized_sentance, all_words):\n",
    "    tokenized_sentance = [lemm(word) for word in tokenized_sentance]\n",
    "    bag = np.zeros(len(all_words), dtype=np.float32)\n",
    "    for index_of_the_word, word in enumerate(all_words):\n",
    "        if word in tokenized_sentance:\n",
    "            bag[index_of_the_word] = 1.0\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa5d016-c987-43f7-9b29-c532c2b89e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentance = \"this is bullshit I can't believe that this is how you do natural language processing, I thought there might be more steps!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886112d-b8c7-4b9e-89ea-22a61692d823",
   "metadata": {},
   "source": [
    "#### here we added the tags in the josn file into a list \n",
    "#### we've also made a nested loop to tokenize the patterns in each tag and add it to 'all_words' list\n",
    "#### so it will come to one tag, tokenize it's patterns, and saves it to the 'all_words' list and then go to another tag\n",
    "#### after we did all of that, we then made a list of tuples called xy, the list will work like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8f94fc-5acf-44d2-9dec-2cb0f33a37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[\n",
    "#([a tokenized sentance], the tag it's asscoiated with), \n",
    "# ([another tokenized sentance], the other tag that is associated),\n",
    "# (['hello', 'how', 'are', 'you'], 'greeting'),\n",
    "# (['how', 'old', 'are', 'you'], 'age')\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac3532e4-313e-428a-87b0-2c291b70e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "tags =[]\n",
    "xy = []\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        w = tokenize(pattern)\n",
    "        \n",
    "        # notice here that we have uesd extend, becaues append will add each tokenized pattern as an indvidual list and we don't want that,\n",
    "        # we want a 1D array not a multidimensional array\n",
    "        # append -> ['google'] + ['hello'] = [['google'], ['hello']]\n",
    "        # extedn -> ['google'] + ['hello'] = ['google', 'hello']\n",
    "        \n",
    "        all_words.extend(w)\n",
    "        xy.append((w, tag))\n",
    "\n",
    "# removing punctioation marks\n",
    "ignore_words = [',', '.', '!', '?']\n",
    "clean_words = [word for word in all_words if word not in ignore_words]\n",
    "\n",
    "# lowering words\n",
    "lower_words = [word.lower() for word in clean_words]\n",
    "\n",
    "# sorting words\n",
    "sortd_words = sorted(set(lower_words))\n",
    "\n",
    "# we sorted the tags so we can have an easier job when we do the next step\n",
    "lower_tags = [tag.lower() for tag in tags]\n",
    "sorted_tags = sorted(lower_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54f0f6d-dc0b-475d-8793-91308149b313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'m\", \"'s\", '10', '19', 'a', 'age', 'am', 'anyone', 'are', 'ask', 'awesome', 'bad', 'bbye', 'be', 'best', 'bye', 'can', 'contact', 'could', 'covid', 'creator', 'cricket', 'current', 'date', 'day', 'designed', 'developer', 'do', 'doing', 'dumb', 'fine', 'for', 'funny', 'get', 'good', 'goodbye', 'google', 'great', 'haha', 'he', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'hola', 'hot', 'how', 'i', 'idiot', 'india', 'inspiration', 'inspires', 'internet', 'is', 'it', 'joke', 'karan', 'know', 'later', 'latest', 'laugh', 'lmao', 'lol', 'lost', 'made', 'make', 'malik', 'matches', 'me', 'motivates', 'namaste', 'news', 'next', 'nice', 'no', 'nope', 'offered', 'ok', 'old', 'programmed', 'programmer', 'provide', 'question', 'riddle', 'rofl', 'score', 'search', 'see', 'set', 'shut', 'songs', 'suggest', 'suggestions', 'sup', 'support', 'talk', 'talking', 'tell', 'temperature', 'ten', 'thank', 'thanks', 'that', 'thats', 'the', 'there', 'till', 'time', 'timer', 'to', 'today', 'top', 'up', 'upto', 'useless', 'was', 'wazzup', 'weather', 'were', 'what', 'whats', 'when', 'who', 'yeah', 'yo', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "print(sortd_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a9318cd-058a-4755-9558-bfae580b41e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['activity', 'age', 'appreciate', 'contact', 'covid19', 'cricket', 'datetime', 'exclaim', 'goodbye', 'google', 'greeting', 'greetreply', 'haha', 'identity', 'inspire', 'insult', 'jokes', 'karan', 'news', 'nicetty', 'no', 'noanswer', 'options', 'programmer', 'riddle', 'song', 'suggest', 'thanks', 'timer', 'weather', 'whatsup']\n"
     ]
    }
   ],
   "source": [
    "print(sorted_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf261844-355c-46c4-b941-5b2373a22b88",
   "metadata": {},
   "source": [
    "# after all that mumbo jumbo let's create a bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48d5b7-9e71-4f2c-9847-3d0ae2949f32",
   "metadata": {},
   "source": [
    "### we use the all_words list just as a reference so we can give each word a BoW\n",
    "### let's imagine this is the current BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d32093-028a-4400-88da-8cc77fd0328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words -> ['hello', 'bye', 'thanks', 'time']\n",
    "# bow_words -> [1,1,1,1]\n",
    "# random_tokenized_string -> ['hello', 'my', 'dear']\n",
    "# throwing the string in the bow it will give us\n",
    "# [1,0,0,0]\n",
    "# we can figure out that this string is greating since it match the 'hello', in the bow\n",
    "# this is all according to my understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38db2e9-5336-4e73-8904-6ac485752368",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train =[]\n",
    "\n",
    "for (pattern_sentance, tag) in xy:\n",
    "    bag = bag_of_words(pattern_sentance, all_words)\n",
    "    x_train.append(bag)\n",
    "\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label) # this is basiclly one_hot_encoding the tags or as it called in pytorch CrossEntropyLoss\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.astype('longlong')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1f1d1-28c3-4db6-a70a-2d8567789b6d",
   "metadata": {},
   "source": [
    "# and now for some PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94a2091e-88ad-4500-9219-a90cdd158641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38af3bbb-2a6f-4d4f-93ad-080bea910b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(x_train)\n",
    "        self.x_data = x_train\n",
    "        self.y_data = y_train\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc06a12d-daa6-48ab-ba33-cd852814d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "795a7a38-da71-4440-90f8-2c1c81d4ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef22bfc-3512-49c8-8a1d-bc039a09bb18",
   "metadata": {},
   "source": [
    "### we will want to create a new class for the neural network part of the chatbot, the first class was for the data, and now this one is for the machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d103dd1a-9a23-4b50-808c-ce332f93997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this little bad boy is our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdc07a13-c004-4aac-a49c-0d49a23c6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation, no selfmax\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "281df57c-4db0-4a06-8463-44046f415085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more hyperparameters\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "input_size = len(x_train[0])\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d8b47d1-dbf2-486d-b44c-7a28e9da9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "671448b9-c8d7-44d3-90df-df2fcfb7bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimize\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29a18a56-ee86-4e8a-a323-406b4bfa55a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100/1000, loss = 1.6618\n",
      "epoch: 200/1000, loss = 0.0211\n",
      "epoch: 300/1000, loss = 0.0234\n",
      "epoch: 400/1000, loss = 0.0014\n",
      "epoch: 500/1000, loss = 0.0001\n",
      "epoch: 600/1000, loss = 0.0031\n",
      "epoch: 700/1000, loss = 0.0432\n",
      "epoch: 800/1000, loss = 0.0344\n",
      "epoch: 900/1000, loss = 0.0000\n",
      "epoch: 1000/1000, loss = 0.0000\n",
      "final loss, loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # forward\n",
    "        output = model(words)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # backward and optimizer steps\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'epoch: {epoch+1}/{num_epochs}, loss = {loss.item():.4f}')\n",
    "print(f'final loss, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4da8e7f-b940-4f3e-b8d5-045303a6330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdf33380-2aaa-419e-967c-a90ff2a35e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"all_words\": all_words,\n",
    "    \"tags\": tags\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1491c273-8ed5-48db-96a0-45cac31a73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"data.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbe3c291-6229-4efd-a7f8-92e3f0fe6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa9a3dfc-d722-4a75-956d-c1bfea209f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete and file saved in data.pth\n"
     ]
    }
   ],
   "source": [
    "print(f'training complete and file saved in {FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca880cd7-a946-4edc-b7f4-fd775579a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 4 python files in this project but I've placed all of their functions inside this notebook\n",
    "# intents.json\n",
    "# train.py\n",
    "# chat.py\n",
    "# model.py\n",
    "# nltk_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e303fb8-c4b7-4425-a4b8-c25a45a5b21a",
   "metadata": {},
   "source": [
    "# building the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eedb9c79-d98b-4da1-94dd-c40ab71a82ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=276, out_features=8, bias=True)\n",
       "  (l2): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (l3): Linear(in_features=8, out_features=31, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with open('intents.json', 'r') as f:\n",
    "    intetns = json.load(f)\n",
    "    \n",
    "FILE = 'data.pth'\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data['input_size']\n",
    "hidden_size = data['hidden_size']\n",
    "output_size = data['output_size']\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data['model_state']\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79134ffb-5ad3-4eec-afb6-2c89af569166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let's chat!, type quit to exit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms chat!, type quit to exit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     sentance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentance\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "bot_name = 'Bot'\n",
    "print(\"let's chat!, type quit to exit\")\n",
    "while True:\n",
    "    sentance = input(\"You: \")\n",
    "    if sentance=='quit':\n",
    "        break\n",
    "    sentance = tokenize(sentance)\n",
    "    x = bag_of_words(sentance, all_words)\n",
    "    x = x.reshape(1, x.shape[0])\n",
    "    x = torch.from_numpy(x)\n",
    "\n",
    "    output = model(x)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "\n",
    "    if prob.item() > 0.4:\n",
    "        for intent in intetns['intents']:\n",
    "            if tag==intent['tag']:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "    else:\n",
    "        print(f'{bot_name}: I do not understand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9dfd50b2-a231-437d-83a6-e2fc5024c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = json.loads(open('written_intents.json').read())\n",
    "df = pd.json_normalize(file['intents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5562b63c-76ec-4968-87b9-b0ad8e96e062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>patterns</th>\n",
       "      <th>responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greeting</td>\n",
       "      <td>[hello, hi, hey, what's up, anybody here, good...</td>\n",
       "      <td>[Hello!, Hi there, how can I help you today?, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>[good bye, adios, see you later, bye]</td>\n",
       "      <td>[Come back soon!, Good bye!, Have a nice day!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thanks</td>\n",
       "      <td>[thank you, thanks, thanks for help, that's he...</td>\n",
       "      <td>[Happy to help!, You're welcome!, Feel free to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nothing</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Please provide more context, Not sure I under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>options</td>\n",
       "      <td>[How you could help me?, What you can do?, Wha...</td>\n",
       "      <td>[I am a data science chatbot. My capabilities ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jokes</td>\n",
       "      <td>[Tell me a joke, Joke, Make me laugh, tell me ...</td>\n",
       "      <td>[A perfectionist walked into a bar...apparentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Identity</td>\n",
       "      <td>[Who are you, what are you]</td>\n",
       "      <td>[I am a chatbot that was made to provide you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>programmer</td>\n",
       "      <td>[Who made you, who designed you, who programme...</td>\n",
       "      <td>[I was made by Zaid and Layan., My creators ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>activity</td>\n",
       "      <td>[what are you doing, what are you upto]</td>\n",
       "      <td>[Talking to you, of course!, Just chatting wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>contact</td>\n",
       "      <td>[contact developer, contact layan, contact pro...</td>\n",
       "      <td>[You can contact my creators at theirs Linkedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>data</td>\n",
       "      <td>[what is data, where can I get data, how is da...</td>\n",
       "      <td>[Data is facts and statistics collected togeth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>data types</td>\n",
       "      <td>[tell me more about data, what are data types,...</td>\n",
       "      <td>[Data has many types like structured, unstrutc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data engineering</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data analysis</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>salary</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>machine learning</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>data cleaning</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tag                                           patterns  \\\n",
       "0           greeting  [hello, hi, hey, what's up, anybody here, good...   \n",
       "1            goodbye              [good bye, adios, see you later, bye]   \n",
       "2             thanks  [thank you, thanks, thanks for help, that's he...   \n",
       "3            nothing                                                 []   \n",
       "4            options  [How you could help me?, What you can do?, Wha...   \n",
       "5              jokes  [Tell me a joke, Joke, Make me laugh, tell me ...   \n",
       "6           Identity                        [Who are you, what are you]   \n",
       "7         programmer  [Who made you, who designed you, who programme...   \n",
       "8           activity            [what are you doing, what are you upto]   \n",
       "9            contact  [contact developer, contact layan, contact pro...   \n",
       "10              data  [what is data, where can I get data, how is da...   \n",
       "11        data types  [tell me more about data, what are data types,...   \n",
       "12  data engineering                                                 []   \n",
       "13     data analysis                                                 []   \n",
       "14            salary                                                 []   \n",
       "15  machine learning                                                 []   \n",
       "16     data cleaning                                                 []   \n",
       "17                                                                   []   \n",
       "18                                                                   []   \n",
       "\n",
       "                                            responses  \n",
       "0   [Hello!, Hi there, how can I help you today?, ...  \n",
       "1      [Come back soon!, Good bye!, Have a nice day!]  \n",
       "2   [Happy to help!, You're welcome!, Feel free to...  \n",
       "3   [Please provide more context, Not sure I under...  \n",
       "4   [I am a data science chatbot. My capabilities ...  \n",
       "5   [A perfectionist walked into a bar...apparentl...  \n",
       "6   [I am a chatbot that was made to provide you w...  \n",
       "7   [I was made by Zaid and Layan., My creators ar...  \n",
       "8   [Talking to you, of course!, Just chatting wit...  \n",
       "9   [You can contact my creators at theirs Linkedi...  \n",
       "10  [Data is facts and statistics collected togeth...  \n",
       "11  [Data has many types like structured, unstrutc...  \n",
       "12                                                 []  \n",
       "13                                                 []  \n",
       "14                                                 []  \n",
       "15                                                 []  \n",
       "16                                                 []  \n",
       "17                                                 []  \n",
       "18                                                 []  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2955f42-3728-44ff-add3-f0ecf3cd8193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
