{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55b75170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65437d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =open('Layan.json').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c0e1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=json.loads(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3de62bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['hello', 'hi', 'hey', \"what's up\", 'anybody here', 'good day'],\n",
       "   'responses': ['Hello!',\n",
       "    'Hi there, how can I help you today?',\n",
       "    'Feel free to ask me anything',\n",
       "    'Ask me about Data Science!']},\n",
       "  {'tag': 'Target User',\n",
       "   'patterns': [\"who's a Target user for the chatbot'\",\n",
       "    'for who they create you'],\n",
       "   'responses': ['Beginners in the field',\n",
       "    'First year students',\n",
       "    'those who want to start in the field']},\n",
       "  {'tag': 'thanks',\n",
       "   'patterns': ['thank you',\n",
       "    'thanks',\n",
       "    'thanks for help',\n",
       "    \"that's helpful\",\n",
       "    'you are a greet help',\n",
       "    'awesome',\n",
       "    'very helpful'],\n",
       "   'responses': ['Happy to help!',\n",
       "    \"You're welcome!\",\n",
       "    'Feel free to ask me again',\n",
       "    'Anytime!',\n",
       "    'My pleasure']},\n",
       "  {'tag': 'nothing',\n",
       "   'patterns': [],\n",
       "   'responses': ['Please provide more context',\n",
       "    'Not sure I understand',\n",
       "    \"Sorry, I can't understand you\"]},\n",
       "  {'tag': 'options',\n",
       "   'patterns': ['How you could help me?',\n",
       "    'What you can do?',\n",
       "    'What help you provide?',\n",
       "    'How you can be helpful?',\n",
       "    'What support is offered'],\n",
       "   'responses': ['I am a data science chatbot. My capabilities are : I can chat with you and teach you about data science.']},\n",
       "  {'tag': 'jokes',\n",
       "   'patterns': ['Tell me a joke',\n",
       "    'Joke',\n",
       "    'Make me laugh',\n",
       "    'tell me something funny'],\n",
       "   'responses': [\"A perfectionist walked into a bar...apparently, the bar wasn't set high enough\",\n",
       "    'I ate a clock yesterday, it was very time-consuming',\n",
       "    \"Never criticize someone until you've walked a mile in their shoes. That way, when you criticize them, they won't be able to hear you from that far away. Plus, you'll have their shoes.\",\n",
       "    \"The world tongue-twister champion just got arrested. I hear they're gonna give him a really tough sentence.\",\n",
       "    \"I own the world's worst thesaurus. Not only is it awful, it's awful.\",\n",
       "    'What did the traffic light say to the car? \"Don\\'t look now, I\\'m changing.\"',\n",
       "    'What do you call a snowman with a suntan? A puddle.',\n",
       "    'How does a penguin build a house? Igloos it together',\n",
       "    'I went to see the doctor about my short-term memory problems â€“ the first thing he did was make me pay in advance',\n",
       "    'As I get older and I remember all the people Iâ€™ve lost along the way, I think to myself, maybe a career as a tour guide wasnâ€™t for me.',\n",
       "    \"o what if I don't know what 'Armageddon' means? It's not the end of the world.\"]},\n",
       "  {'tag': 'Identity',\n",
       "   'patterns': ['Who are you', 'what are you'],\n",
       "   'responses': ['I am a chatbot that was made to provide you with info about data science',\n",
       "    \"I am a data science chatbot, I was made to give you info about data science and it's related fields\"]},\n",
       "  {'tag': 'creator',\n",
       "   'patterns': ['Who made you', 'who designed you', 'who programmed you'],\n",
       "   'responses': ['I was made by Zaid and Layan.',\n",
       "    'My creators are Layan and Zaid, they are Data Science students from BAU']},\n",
       "  {'tag': 'activity',\n",
       "   'patterns': ['what are you doing', 'what are you upto'],\n",
       "   'responses': ['Talking to you, of course!', 'Just chatting with you!']},\n",
       "  {'tag': 'contact',\n",
       "   'patterns': ['contact developer',\n",
       "    'contact layan',\n",
       "    'contact programmer',\n",
       "    'contact creator',\n",
       "    'contact zaid',\n",
       "    'speak to developers',\n",
       "    'talk to programmers'],\n",
       "   'responses': ['You can contact my creators at theirs Linkedin profiles Zaid : https://www.linkedin.com/in/zaid-allwansah-a09412227/ \\n Layan : https://www.linkedin.com/in/layan-bilbeisi/']},\n",
       "  {'tag': 'data',\n",
       "   'patterns': ['what is data',\n",
       "    'where can I get data',\n",
       "    'how is data used',\n",
       "    'data'],\n",
       "   'responses': ['Data is facts and statistics collected together for reference or analysis, data is collected by a lot of different sources, like the web or sensory devices']},\n",
       "  {'tag': 'data types',\n",
       "   'patterns': ['tell me more about data',\n",
       "    'what are data types',\n",
       "    'different types of data'],\n",
       "   'responses': ['Data has many types like structured, unstrutcured and semi-structured, one of the most used data types is sturcured data, structured data refers to data that resides in a fixed field within a file or record. Like spreadsheets or excel files!']},\n",
       "  {'tag': 'Data Science',\n",
       "   'patterns': ['what is data science?',\n",
       "    'tell me about data science',\n",
       "    'data science',\n",
       "    'explain data science'],\n",
       "   'responses': ['Data science is a multidisciplinary field that involves the use of scientific methods, processes, algorithms, and systems to extract meaningful insights and knowledge from structured and unstructured data.']},\n",
       "  {'tag': 'data engineering',\n",
       "   'patterns': ['whta is data engineering',\n",
       "    'tell me about data engineering',\n",
       "    'defention of data engineering',\n",
       "    'data engineering'],\n",
       "   'responses': ['Data engineering is a field within data science that focuses on the practical application of data collection and processing']},\n",
       "  {'tag': 'data analysis',\n",
       "   'patterns': ['what is data analysis',\n",
       "    'defention of data analysis',\n",
       "    'explain data analysis',\n",
       "    'tell me about data analysis',\n",
       "    'data analysis'],\n",
       "   'responses': ['Data analysis is the process of inspecting, cleaning, transforming, and modeling data with the goal of discovering useful information, drawing conclusions, and supporting decision-making.']},\n",
       "  {'tag': 'salary',\n",
       "   'patterns': ['salary for data science', 'range of salary for data science'],\n",
       "   'responses': ['in USA Entry-Level: $50,000 - $70,000 Mid-Level: $70,000 - $90,000 Senior Level: $90,000 - $120,000+']},\n",
       "  {'tag': 'machine learning',\n",
       "   'patterns': ['what is machine learning',\n",
       "    'defention of machine learning',\n",
       "    'explain machine learning',\n",
       "    'tell me more about machine learning',\n",
       "    'machine learning'],\n",
       "   'responses': ['Machine learning is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computer systems to perform tasks without explicit programming.']},\n",
       "  {'tag': 'data cleaning',\n",
       "   'patterns': ['what is data cleaning',\n",
       "    'how to do data cleaning',\n",
       "    'data cleaning'],\n",
       "   'responses': ['Data cleaning is a crucial step in the data analysis and data science process. It involves identifying and correcting errors or inconsistencies in datasets to improve their quality and reliability']},\n",
       "  {'tag': 'machine learning algorthim',\n",
       "   'patterns': ['what is the machine learning algorthim',\n",
       "    'algorthims to machine learning'],\n",
       "   'responses': ['these are some popular algorthims: Linear Regression,Random forest,KNN,Decision tree']},\n",
       "  {'tag': 'data preporcessing',\n",
       "   'patterns': ['what is data preprocessing', 'how to do it'],\n",
       "   'responses': [' is a crucial step in the data analysis and machine learning pipeline. It involves cleaning and transforming raw data into a format that is suitable for analysis or for training machine learning models.']},\n",
       "  {'tag': 'Feature engineering',\n",
       "   'patterns': ['what is Feature engineering',\n",
       "    'tell me about Feature engineering',\n",
       "    'Feature engineering'],\n",
       "   'responses': ['Feature engineering is the process of creating new features or modifying existing ones to enhance the performance of machine learning models. It involves selecting, transforming, and creating features that provide meaningful and relevant information for the task at hand']},\n",
       "  {'tag': 'Model Evaluation Metrics',\n",
       "   'patterns': ['what is Model Evaluation Metrics',\n",
       "    'tell me about Model Evaluation Metrics',\n",
       "    'explain Model Evaluation Metrics'],\n",
       "   'responses': ['Model evaluation metrics are used to assess the performance of machine learning models such as accuracy.']},\n",
       "  {'tag': 'Supervised',\n",
       "   'patterns': ['what is supervised',\n",
       "    'is supervised type of machine learning',\n",
       "    'explain supervised'],\n",
       "   'responses': ['Supervised refers to a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the input data used for training is paired with corresponding output labels.']},\n",
       "  {'tag': 'Unsupervised',\n",
       "   'patterns': ['what is unssupervised',\n",
       "    'it is a part of unsupervised',\n",
       "    'explain unsupervised',\n",
       "    'is unsupervised type of machine learning'],\n",
       "   'responses': ['Unsupervised learning is a type of machine learning where the algorithm is trained on data without explicit labels or output categories. In other words, the algorithm explores the inherent structure in the input data without the guidance of predefined output labels.']},\n",
       "  {'tag': 'Reinforcement',\n",
       "   'patterns': ['what is Reinforcement',\n",
       "    'it is a part of Reinforcement',\n",
       "    'explain Reinforcement',\n",
       "    'is Reinforcement type of machine learning'],\n",
       "   'responses': ['Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment.']},\n",
       "  {'tag': 'cross validation',\n",
       "   'patterns': ['what is cross validation', 'explain cross validation'],\n",
       "   'responses': ['Cross-validation is a statistical technique used in machine learning to assess the performance and generalizability of a predictive model.']},\n",
       "  {'tag': 'ensemble learning',\n",
       "   'patterns': ['what is ensemble learning',\n",
       "    'tell me more about ensemble learning'],\n",
       "   'responses': ['Ensemble learning is a machine learning technique that combines the predictions of multiple individual models (learners) to improve overall performance and generalization']},\n",
       "  {'tag': 'hyperparameter tuning',\n",
       "   'patterns': ['what is hyperparameter tuning',\n",
       "    'tell me about hyperparameter tuning',\n",
       "    'explain hyperparameter tuning',\n",
       "    'hyperparameter tuning'],\n",
       "   'responses': ['Hyperparameter tuning, also known as hyperparameter optimization or model selection, is the process of finding the best set of hyperparameters for a machine learning model. ']},\n",
       "  {'tag': 'data visualization',\n",
       "   'patterns': ['what is data visualization',\n",
       "    'tell me more about data visualization',\n",
       "    'explain data visualization',\n",
       "    'data visualization'],\n",
       "   'responses': ['Data visualization is the representation of data in graphical or pictorial formats to help people understand, interpret, and derive insights from complex datasets.']},\n",
       "  {'tag': 'Programming languages',\n",
       "   'patterns': ['what is Programming languages use in data science',\n",
       "    'Programming languages in data science',\n",
       "    'what popluar Programming languages in data science'],\n",
       "   'responses': ['python and R']},\n",
       "  {'tag': 'big data technologies',\n",
       "   'patterns': ['what is the big data technologies', 'big data technologies'],\n",
       "   'responses': ['KNime', 'Hadoop', 'rapid Miner', 'Apache Spark']},\n",
       "  {'tag': 'popular libraries and Framework',\n",
       "   'patterns': ['what is popular libraries and Framework',\n",
       "    'most popular libraries in data science',\n",
       "    'what library uses for data science'],\n",
       "   'responses': ['python (numpy, pandas, matplotlib,seaborn,scikit-learn',\n",
       "    'TensorFlow']},\n",
       "  {'tag': 'data Science tools',\n",
       "   'patterns': ['the most important tools for data science',\n",
       "    'what tools for data science',\n",
       "    'data Science tools'],\n",
       "   'responses': ['Jupyter notebook',\n",
       "    'SQl (oracle sql, microsoft sql server, postragl sql),knime, Power bi, Tableau']},\n",
       "  {'tag': 'blog for data science',\n",
       "   'patterns': ['what is the best blog in data science',\n",
       "    'how to know the new in data science',\n",
       "    'blog for data science'],\n",
       "   'responses': ['https://www.smartdatacollective.com/',\n",
       "    'https://whatsthebigdata.com/',\n",
       "    'https://medium.com/kaggle-blog']},\n",
       "  {'tag': 'Natural Language processing',\n",
       "   'patterns': ['what is Natural Language processing',\n",
       "    'explain Natural Language processing',\n",
       "    'what is NLP',\n",
       "    'Natural Language processing',\n",
       "    'NLP'],\n",
       "   'responses': ['Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. The goal of NLP is to enable machines to understand, interpret, and generate human language in a way that is both meaningful and contextually relevant.']},\n",
       "  {'tag': 'Time Series Analysis',\n",
       "   'patterns': ['what is Time Series Analysis',\n",
       "    'explain Time Series Analysis',\n",
       "    'defention of Time Series Analysis',\n",
       "    'Time Series'],\n",
       "   'responses': ['Time series analysis is a statistical technique that deals with time-ordered data points. It involves studying the patterns, trends, and behaviors in sequential data to make predictions or gain insights into the underlying processes.']},\n",
       "  {'tag': 'Clustring Algorthims',\n",
       "   'patterns': ['what is Clustring Algorthims',\n",
       "    'explain Clustring Algorthims',\n",
       "    'tell me about Clustring Algorthims',\n",
       "    'Clustring Algorthims'],\n",
       "   'responses': ['Clustering algorithms are used in unsupervised machine learning to group similar data points together based on certain criteria. The goal of clustering is to identify patterns or structures in the data without explicit labels.']},\n",
       "  {'tag': 'Dimensionality Reduction',\n",
       "   'patterns': ['what is Dimensionality Reduction',\n",
       "    'tell me about Dimensionality Reduction',\n",
       "    'explain Dimensionality Reduction'],\n",
       "   'responses': ['Dimensionality reduction is a technique used in machine learning and data analysis to reduce the number of input features or variables in a dataset. The goal is to simplify the dataset while retaining its important characteristics and reducing computational complexity.']},\n",
       "  {'tag': 'Data Science project Lifecycle',\n",
       "   'patterns': ['what is a lifecycle for data science project',\n",
       "    'project in data science',\n",
       "    'what is the steps for data science project'],\n",
       "   'responses': ['1.Define Objectives and Scope 2.Data Collection  3.Data Cleaning and Preprocessing  4.Exploratory Data Analysis (EDA)  5.Feature Engineering  6.Model Development 7.Model Evaluation']},\n",
       "  {'tag': 'kaggle for data science',\n",
       "   'patterns': ['what is kaggle',\n",
       "    'platform for data science',\n",
       "    'where to get data',\n",
       "    'kaggle compition'],\n",
       "   'responses': ['Kaggle is a popular platform for data science competitions, collaborative projects, and learning https://www.kaggle.com/']},\n",
       "  {'tag': 'Data Warehousing',\n",
       "   'patterns': ['what is Data Warehousing',\n",
       "    'Data Warehousing',\n",
       "    'explain Data Warehousing',\n",
       "    'tell me about Data Warehousing'],\n",
       "   'responses': ['Data warehousing is a process of collecting, storing, and managing large volumes of data from various sources to support business intelligence (BI) and analytical reporting.']},\n",
       "  {'tag': 'IDE for data science',\n",
       "   'patterns': ['what is the most popular IDE for data science',\n",
       "    'how to donlowd IDE for data science'],\n",
       "   'responses': ['Jupyter Notebooks: https://www.anaconda.com/',\n",
       "    'colab: https://colab.research.google.com/']},\n",
       "  {'tag': 'EDA',\n",
       "   'patterns': ['what is EDA',\n",
       "    'what is Explorty data analysis',\n",
       "    'EDA',\n",
       "    'tell me about EDA'],\n",
       "   'responses': ['EDA stands for Exploratory Data Analysis, which is a crucial step in the data analysis process. EDA involves the initial exploration and understanding of a dataset to uncover patterns, trends, relationships, and anomalies. ']},\n",
       "  {'tag': 'statistical Description',\n",
       "   'patterns': ['what is statistical Description',\n",
       "    'tell me about statistical Description',\n",
       "    'statistical Description'],\n",
       "   'responses': ['Statistical description refers to the use of statistical measures and techniques to summarize, describe, and interpret the main features of a dataset.']},\n",
       "  {'tag': 'Feature Scaling',\n",
       "   'patterns': ['what is Feature Scaling',\n",
       "    'tell me about Feature Scaling',\n",
       "    'explain Feature Scaling',\n",
       "    'Feature Scaling'],\n",
       "   'responses': ['Feature scaling is a preprocessing step in machine learning that involves transforming the features of a dataset to a standardized range.']},\n",
       "  {'tag': 'Handling missing data',\n",
       "   'patterns': ['how to handle missing data', 'explain Handling missing data'],\n",
       "   'responses': ['1. Dropping Missing Values  2.Imputation  3.Forward Fill and Backward Fill  4.Interpolation  5. Creating a Missing Indicator']},\n",
       "  {'tag': 'Deep learning',\n",
       "   'patterns': ['what is Deep learning',\n",
       "    'tell me more about Deep learning',\n",
       "    'Deep learning',\n",
       "    'defention of deep learning'],\n",
       "   'responses': ['Deep learning is a subfield of machine learning that focuses on artificial neural networks and deep neural networks. It involves training complex models, often with multiple layers, to learn hierarchical representations of data']},\n",
       "  {'tag': 'skills for data science',\n",
       "   'patterns': ['what skills need for data science',\n",
       "    'skills for data science'],\n",
       "   'responses': ['1.Programming Languages   2.Statistical Knowledge  3.Data Manipulation and Analysis  4.Data Visualization 5. Machine Learning  6. SQL Database Knowledge  7. Big Data Technologies  8. Data Cleaning and Preprocessing  9. Feature Engineering']},\n",
       "  {'tag': 'problem solving skills',\n",
       "   'patterns': ['problem solving skills for data science', 'problem solving'],\n",
       "   'responses': ['Problem-solving skills are crucial for data scientists, as they play a key role in formulating and addressing complex challenges using data-driven approaches.']},\n",
       "  {'tag': 'SQL',\n",
       "   'patterns': ['SQL in data science', 'it is sql important for data science'],\n",
       "   'responses': ['In data science, SQL is commonly used for querying, filtering, aggregating, and transforming data stored in relational databases.']},\n",
       "  {'tag': 'statistics',\n",
       "   'patterns': ['statistics in data science',\n",
       "    'what is statistics in data science'],\n",
       "   'responses': ['Statistics is a branch of mathematics that involves the collection, analysis, interpretation, presentation, and organization of data. ']},\n",
       "  {'tag': 'Data Mining',\n",
       "   'patterns': ['what is Data Mining',\n",
       "    'tell me more about Data Mining',\n",
       "    'data mining',\n",
       "    'defention of Data Mining'],\n",
       "   'responses': ['Data mining is the process of discovering patterns, trends, correlations, or valuable information from large datasets using various methods, including machine learning, statistical analysis, and database systems.']},\n",
       "  {'tag': 'Web Scraping',\n",
       "   'patterns': ['what is Web Scraping', 'Web Scraping in data science'],\n",
       "   'responses': ['Web scraping is the process of extracting data from websites. It involves fetching the HTML of a web page, parsing it, and extracting the desired information.']},\n",
       "  {'tag': 'Data Engineering',\n",
       "   'patterns': ['what is Data Engineering',\n",
       "    'tell me about Data Engineering',\n",
       "    'explain Data Engineering'],\n",
       "   'responses': ['Data engineering is a field of study and practice that focuses on designing, developing, and managing the architecture, tools, and systems for collecting, storing, processing, and analyzing data.']},\n",
       "  {'tag': 'company in data science',\n",
       "   'patterns': ['what is the company work in data science in jordan'],\n",
       "   'responses': ['all banks in jordan',\n",
       "    'Zain',\n",
       "    'Orange',\n",
       "    'Umniah',\n",
       "    'Shai for Ai',\n",
       "    'big 4 organization (pwc, E&Y, KMPG, Deloitte ']},\n",
       "  {'tag': 'Data Analysis',\n",
       "   'patterns': ['what is Data Analysis',\n",
       "    'tell me about Data Analysis',\n",
       "    'explain Data Analysis'],\n",
       "   'responses': ['Data analysis is the process of inspecting, cleaning, transforming, and modeling data to discover useful information, draw conclusions, and support decision-making.']},\n",
       "  {'tag': 'Youtube Channels Data analysis',\n",
       "   'patterns': ['what is the Youtube Channels in data analysis',\n",
       "    'best youtube channels in data analysis '],\n",
       "   'responses': ['https://www.youtube.com/@AlexTheAnalyst , https://www.youtube.com/@codebasics']},\n",
       "  {'tag': 'Youtube Channels Python',\n",
       "   'patterns': ['what is the Youtube Channels to learn python',\n",
       "    'best youtube channels in python '],\n",
       "   'responses': ['https://www.youtube.com/watch?v=_uQrJ0TkZlc&t=173s']},\n",
       "  {'tag': 'Youtube Channels Machine learning',\n",
       "   'patterns': ['what is the Youtube Channels to learn Machine learning',\n",
       "    'best youtube channels in machine learning '],\n",
       "   'responses': ['https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU']}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d95d53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "data_x = [] \n",
    "data_y = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        words.extend(tokens)\n",
    "        data_x.append(pattern)\n",
    "        data_y.append(intent[\"tag\"])\n",
    "\n",
    "    if intent[\"tag\"] not in classes:\n",
    "        classes.append(intent[\"tag\"])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51cad813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Your previous code\n",
    "out_empty = [0] * len(classes)\n",
    "training = []\n",
    "\n",
    "for idx, doc in enumerate(data_x):\n",
    "    bow = [] \n",
    "    text = lemmatizer.lemmatize(doc.lower())\n",
    "    for word in words:\n",
    "        if word in text:\n",
    "            bow.append(1)\n",
    "        else:\n",
    "            bow.append(0)\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes.index(data_y[idx])] = 1\n",
    "    \n",
    "    training.append((bow, output_row))\n",
    "\n",
    "random.shuffle(training)\n",
    "\n",
    "train_x = [item[0] for item in training]\n",
    "train_y = [item[1] for item in training]\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb52a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 128)               20608     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 59)                3835      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32699 (127.73 KB)\n",
      "Trainable params: 32699 (127.73 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation=\"softmax\"))  # Assuming this is a multi-class classification task\n",
    "\n",
    "# Use the legacy optimizer\n",
    "adam = tf.keras.optimizers.legacy.Adam(learning_rate=0.01, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "954fc9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "6/6 [==============================] - 1s 5ms/step - loss: 4.1155 - accuracy: 0.0213\n",
      "Epoch 2/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9207 - accuracy: 0.0691\n",
      "Epoch 3/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.7213 - accuracy: 0.1543\n",
      "Epoch 4/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.4548 - accuracy: 0.1383\n",
      "Epoch 5/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1556 - accuracy: 0.1862\n",
      "Epoch 6/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.7217 - accuracy: 0.3351\n",
      "Epoch 7/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5085 - accuracy: 0.3351\n",
      "Epoch 8/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.1673 - accuracy: 0.4309\n",
      "Epoch 9/150\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.8438 - accuracy: 0.4415\n",
      "Epoch 10/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.7873 - accuracy: 0.4787\n",
      "Epoch 11/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4747 - accuracy: 0.5691\n",
      "Epoch 12/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.4240 - accuracy: 0.5638\n",
      "Epoch 13/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3091 - accuracy: 0.6117\n",
      "Epoch 14/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.1692 - accuracy: 0.6809\n",
      "Epoch 15/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2361 - accuracy: 0.6330\n",
      "Epoch 16/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9897 - accuracy: 0.7287\n",
      "Epoch 17/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0243 - accuracy: 0.7234\n",
      "Epoch 18/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8806 - accuracy: 0.7287\n",
      "Epoch 19/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7920 - accuracy: 0.7447\n",
      "Epoch 20/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8795 - accuracy: 0.6968\n",
      "Epoch 21/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9376 - accuracy: 0.7021\n",
      "Epoch 22/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7358 - accuracy: 0.7500\n",
      "Epoch 23/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7121 - accuracy: 0.7660\n",
      "Epoch 24/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6012 - accuracy: 0.7872\n",
      "Epoch 25/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5304 - accuracy: 0.8191\n",
      "Epoch 26/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6906 - accuracy: 0.7979\n",
      "Epoch 27/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4949 - accuracy: 0.8511\n",
      "Epoch 28/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6384 - accuracy: 0.7766\n",
      "Epoch 29/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.6866 - accuracy: 0.7500\n",
      "Epoch 30/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5682 - accuracy: 0.8191\n",
      "Epoch 31/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6411 - accuracy: 0.7606\n",
      "Epoch 32/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5403 - accuracy: 0.8032\n",
      "Epoch 33/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5536 - accuracy: 0.8404\n",
      "Epoch 34/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5616 - accuracy: 0.8404\n",
      "Epoch 35/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6336 - accuracy: 0.8085\n",
      "Epoch 36/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5867 - accuracy: 0.8351\n",
      "Epoch 37/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6101 - accuracy: 0.7819\n",
      "Epoch 38/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5134 - accuracy: 0.8245\n",
      "Epoch 39/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4592 - accuracy: 0.8564\n",
      "Epoch 40/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6287 - accuracy: 0.7766\n",
      "Epoch 41/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4424 - accuracy: 0.8511\n",
      "Epoch 42/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5641 - accuracy: 0.8404\n",
      "Epoch 43/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4599 - accuracy: 0.8351\n",
      "Epoch 44/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3758 - accuracy: 0.8670\n",
      "Epoch 45/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4410 - accuracy: 0.8298\n",
      "Epoch 46/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4918 - accuracy: 0.8457\n",
      "Epoch 47/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4151 - accuracy: 0.8723\n",
      "Epoch 48/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4129 - accuracy: 0.8564\n",
      "Epoch 49/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4723 - accuracy: 0.8351\n",
      "Epoch 50/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5072 - accuracy: 0.8191\n",
      "Epoch 51/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5655 - accuracy: 0.8457\n",
      "Epoch 52/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4805 - accuracy: 0.8298\n",
      "Epoch 53/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3560 - accuracy: 0.8564\n",
      "Epoch 54/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.8404\n",
      "Epoch 55/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4263 - accuracy: 0.8351\n",
      "Epoch 56/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4922 - accuracy: 0.8457\n",
      "Epoch 57/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3936 - accuracy: 0.8883\n",
      "Epoch 58/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4007 - accuracy: 0.8404\n",
      "Epoch 59/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4894 - accuracy: 0.8138\n",
      "Epoch 60/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4261 - accuracy: 0.8777\n",
      "Epoch 61/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4538 - accuracy: 0.8564\n",
      "Epoch 62/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5436 - accuracy: 0.8191\n",
      "Epoch 63/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3064 - accuracy: 0.8989\n",
      "Epoch 64/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3952 - accuracy: 0.8617\n",
      "Epoch 65/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6159 - accuracy: 0.7979\n",
      "Epoch 66/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4010 - accuracy: 0.8883\n",
      "Epoch 67/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3724 - accuracy: 0.8830\n",
      "Epoch 68/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3678 - accuracy: 0.8670\n",
      "Epoch 69/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5436 - accuracy: 0.8298\n",
      "Epoch 70/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4442 - accuracy: 0.8617\n",
      "Epoch 71/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3812 - accuracy: 0.8670\n",
      "Epoch 72/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3754 - accuracy: 0.8989\n",
      "Epoch 73/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3545 - accuracy: 0.9096\n",
      "Epoch 74/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.4165 - accuracy: 0.8564\n",
      "Epoch 75/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3759 - accuracy: 0.8564\n",
      "Epoch 76/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5094 - accuracy: 0.8085\n",
      "Epoch 77/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4404 - accuracy: 0.8457\n",
      "Epoch 78/150\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3415 - accuracy: 0.8670\n",
      "Epoch 79/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3479 - accuracy: 0.8830\n",
      "Epoch 80/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3674 - accuracy: 0.8564\n",
      "Epoch 81/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3001 - accuracy: 0.8457\n",
      "Epoch 82/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3275 - accuracy: 0.8777\n",
      "Epoch 83/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3305 - accuracy: 0.8777\n",
      "Epoch 84/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.3730 - accuracy: 0.8670\n",
      "Epoch 85/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4599 - accuracy: 0.8457\n",
      "Epoch 86/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8617\n",
      "Epoch 87/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5178 - accuracy: 0.8457\n",
      "Epoch 88/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3699 - accuracy: 0.8723\n",
      "Epoch 89/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4186 - accuracy: 0.8617\n",
      "Epoch 90/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.2796 - accuracy: 0.8830\n",
      "Epoch 91/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4097 - accuracy: 0.8670\n",
      "Epoch 92/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3083 - accuracy: 0.8936\n",
      "Epoch 93/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3636 - accuracy: 0.8777\n",
      "Epoch 94/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4408 - accuracy: 0.8511\n",
      "Epoch 95/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3878 - accuracy: 0.8457\n",
      "Epoch 96/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3832 - accuracy: 0.8883\n",
      "Epoch 97/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.8404\n",
      "Epoch 98/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.4416 - accuracy: 0.8617\n",
      "Epoch 99/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5960 - accuracy: 0.8245\n",
      "Epoch 100/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4113 - accuracy: 0.8670\n",
      "Epoch 101/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3567 - accuracy: 0.8830\n",
      "Epoch 102/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5140 - accuracy: 0.8511\n",
      "Epoch 103/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3190 - accuracy: 0.8777\n",
      "Epoch 104/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.2951 - accuracy: 0.8883\n",
      "Epoch 105/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.4364 - accuracy: 0.8617\n",
      "Epoch 106/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8777\n",
      "Epoch 107/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.2864 - accuracy: 0.9202\n",
      "Epoch 108/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.3704 - accuracy: 0.8670\n",
      "Epoch 109/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3711 - accuracy: 0.8723\n",
      "Epoch 110/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3052 - accuracy: 0.8936\n",
      "Epoch 111/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4381 - accuracy: 0.8564\n",
      "Epoch 112/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4423 - accuracy: 0.8564\n",
      "Epoch 113/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4146 - accuracy: 0.8883\n",
      "Epoch 114/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3488 - accuracy: 0.8830\n",
      "Epoch 115/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4344 - accuracy: 0.8723\n",
      "Epoch 116/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.2824 - accuracy: 0.8936\n",
      "Epoch 117/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4281 - accuracy: 0.8564\n",
      "Epoch 118/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5371 - accuracy: 0.8085\n",
      "Epoch 119/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4209 - accuracy: 0.8670\n",
      "Epoch 120/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3976 - accuracy: 0.8670\n",
      "Epoch 121/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8670\n",
      "Epoch 122/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5446 - accuracy: 0.8245\n",
      "Epoch 123/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5295 - accuracy: 0.8617\n",
      "Epoch 124/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4048 - accuracy: 0.8670\n",
      "Epoch 125/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3854 - accuracy: 0.8777\n",
      "Epoch 126/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3393 - accuracy: 0.8670\n",
      "Epoch 127/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3880 - accuracy: 0.8777\n",
      "Epoch 128/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.2958 - accuracy: 0.8936\n",
      "Epoch 129/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3222 - accuracy: 0.8830\n",
      "Epoch 130/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4050 - accuracy: 0.8670\n",
      "Epoch 131/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3597 - accuracy: 0.8830\n",
      "Epoch 132/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3164 - accuracy: 0.8723\n",
      "Epoch 133/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4934 - accuracy: 0.8457\n",
      "Epoch 134/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.2878 - accuracy: 0.8830\n",
      "Epoch 135/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3954 - accuracy: 0.8830\n",
      "Epoch 136/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3158 - accuracy: 0.8777\n",
      "Epoch 137/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4560 - accuracy: 0.8511\n",
      "Epoch 138/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4412 - accuracy: 0.8883\n",
      "Epoch 139/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3529 - accuracy: 0.8777\n",
      "Epoch 140/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5130 - accuracy: 0.8670\n",
      "Epoch 141/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3205 - accuracy: 0.8830\n",
      "Epoch 142/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4129 - accuracy: 0.8511\n",
      "Epoch 143/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3134 - accuracy: 0.8936\n",
      "Epoch 144/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5035 - accuracy: 0.8564\n",
      "Epoch 145/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3351 - accuracy: 0.8883\n",
      "Epoch 146/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3256 - accuracy: 0.9043\n",
      "Epoch 147/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.9043\n",
      "Epoch 148/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3606 - accuracy: 0.8670\n",
      "Epoch 149/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4755 - accuracy: 0.8670\n",
      "Epoch 150/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3649 - accuracy: 0.8989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1cd3a9f81f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_x, y=train_y,epochs=150,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f41e8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the Input\n",
    "def clean_text(text):\n",
    "    tokens =nltk.word_tokenize(text)\n",
    "    tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "def bag_of_words(text, vocab):\n",
    "    tokens = clean_text(text)\n",
    "    bow = [0] * len(vocab)\n",
    "    for w in tokens:\n",
    "        for idx, word in enumerate(vocab):\n",
    "            if word ==w:\n",
    "                bow[idx] =1\n",
    "    return np.array(bow)\n",
    "def pred_class(text, vocab, labels):\n",
    "    bow = bag_of_words(text, vocab)\n",
    "    result = model.predict(np.array([bow]))[0]\n",
    "    thresh = 0.5\n",
    "    y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append(labels[r[0]])\n",
    "    return return_list\n",
    "\n",
    "def get_response(intents_list, intents_json):\n",
    "    if len(intents_list)==0:\n",
    "        result=\"Soory! I don't unerstand\"\n",
    "    else:\n",
    "        tag = intents_list[0]\n",
    "        list_of_intents = intents_json[\"intents\"]\n",
    "        for i in list_of_intents:\n",
    "            if i[\"tag\"]==tag:\n",
    "                result=random.choice(i[\"responses\"])\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e99de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# Assuming pred_class and get_response are defined somewhere in your code\n",
    "def pred_class(text, vocab, labels):\n",
    "    bow = bag_of_words(text, vocab)\n",
    "    result = model.predict(np.array([bow]))[0]\n",
    "    thresh = 0.5\n",
    "    y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append(labels[r[0]])\n",
    "    return return_list\n",
    "\n",
    "def get_response(intents_list, intents_json):\n",
    "    if len(intents_list)==0:\n",
    "        result=\"Soory! I don't unerstand\"\n",
    "    else:\n",
    "        tag = intents_list[0]\n",
    "        list_of_intents = intents_json[\"intents\"]\n",
    "        for i in list_of_intents:\n",
    "            if i[\"tag\"]==tag:\n",
    "                result=random.choice(i[\"responses\"])\n",
    "                break\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    st.title(\"Chatbot with Streamlit\")\n",
    "    st.write(\"Press 0 if you don't want to chat with the chatbot\")\n",
    "\n",
    "    while True:\n",
    "        message = st.text_input(\"You: \")\n",
    "        \n",
    "        if message == \"0\":\n",
    "            break\n",
    "        \n",
    "        intents = pred_class(text, vocab, labels)\n",
    "        result = get_response(intents, data)\n",
    "\n",
    "        st.text(f\"Chatbot: {result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482df815",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting altair==4.1.0Note: you may need to restart the kernel to use updated packages.\n",
      "  Downloading altair-4.1.0-py3-none-any.whl (727 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair==4.1.0) (1.26.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair==4.1.0) (0.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair==4.1.0) (0.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair==4.1.0) (3.1.2)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair==4.1.0) (3.2.0)\n",
      "Requirement already satisfied: pandas>=0.18 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair==4.1.0) (2.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.18->altair==4.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.18->altair==4.1.0) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.18->altair==4.1.0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.18->altair==4.1.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->altair==4.1.0) (2.1.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->altair==4.1.0) (0.18.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->altair==4.1.0) (58.0.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->altair==4.1.0) (21.2.0)\n",
      "Installing collected packages: altair\n",
      "  Attempting uninstall: altair\n",
      "    Found existing installation: altair 5.2.0\n",
      "    Uninstalling altair-5.2.0:\n",
      "      Successfully uninstalled altair-5.2.0\n",
      "Successfully installed altair-4.1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install altair==4.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7379209",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.1.4)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: altair in c:\\users\\user\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\user\\anaconda3\\lib\\site-packages (5.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair) (0.3)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair) (3.2.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair) (0.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->altair) (2.1.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->altair) (0.18.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->altair) (58.0.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->altair) (21.2.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pandas altair plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "023945b0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\user\\anaconda3\\lib\\site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\user\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82090f75",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (3.1.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade jinja2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce450a0b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.3)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\USER\\\\anaconda3\\\\Lib\\\\site-packages\\\\~andas\\\\_libs\\\\algos.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.1.4-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "Requirement already satisfied: streamlit in c:\\users\\user\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: tzlocal>=1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (5.2)\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (5.3.1)\n",
      "Requirement already satisfied: pympler>=0.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (1.0.1)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (6.1)\n",
      "Requirement already satisfied: watchdog in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (12.0.0)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (0.8.1b0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (4.8.0)\n",
      "Requirement already satisfied: validators>=0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (0.22.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (10.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (6.7.0)\n",
      "Requirement already satisfied: requests>=2.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (3.1.40)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (13.7.0)\n",
      "Requirement already satisfied: blinker>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (1.7.0)\n",
      "Requirement already satisfied: altair>=3.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (5.2.0)\n",
      "Requirement already satisfied: semver in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (3.0.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (8.0.3)\n",
      "Requirement already satisfied: toml in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click>=7.0->streamlit) (0.4.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19->streamlit) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->streamlit) (3.6.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (21.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (58.0.4)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->altair>=3.2.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.4->streamlit) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.4->streamlit) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.4->streamlit) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.4->streamlit) (2.0.4)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.11.0->streamlit) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.11.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->streamlit) (0.1.2)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pandas streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "decc7952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 0 if you don't want to chat with chatbot\n",
      "hi\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Hi there, how can I help you today?\n",
      "tell me about data analysis\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Data analysis is the process of inspecting, cleaning, transforming, and modeling data with the goal of discovering useful information, drawing conclusions, and supporting decision-making.\n",
      "ide\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Jupyter Notebooks: https://www.anaconda.com/\n",
      "ide\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Jupyter Notebooks: https://www.anaconda.com/\n",
      "bye\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Hello!\n",
      "thank you\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Anytime!\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Press 0 if you don't want to chat with chatbot\")\n",
    "while True:\n",
    "    message = input(\"\")\n",
    "    if message == \"0\":\n",
    "        break\n",
    "    intents = pred_class(message, words, classes)\n",
    "    result = get_response(intents, data)\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bd963b6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlitNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqt5<5.13, but you have pyqt5 5.15.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading streamlit-1.12.0-py2.py3-none-any.whl (9.1 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (1.26.0)\n",
      "Collecting altair>=3.2.0\n",
      "  Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (4.8.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (10.1.0)\n",
      "Collecting gitpython!=3.1.19\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: protobuf<4,>=3.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (5.3.1)\n",
      "Requirement already satisfied: packaging>=14.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (23.1)\n",
      "Collecting tzlocal>=1.1\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (6.7.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (8.0.3)\n",
      "Collecting pydeck>=0.1.dev5\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "Collecting blinker>=1.0.0\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (6.1)\n",
      "Collecting validators>=0.2\n",
      "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests>=2.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: pandas>=0.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (1.5.3)\n",
      "Collecting pympler>=0.9\n",
      "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "Collecting semver\n",
      "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: toml in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: pyarrow>=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (12.0.0)\n",
      "Requirement already satisfied: watchdog in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit) (2.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (3.2.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click>=7.0->streamlit) (0.4.4)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->streamlit) (3.6.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (1.16.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (58.0.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.21.0->streamlit) (2021.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->altair>=3.2.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.4->streamlit) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.4->streamlit) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.4->streamlit) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.4->streamlit) (2023.7.22)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Installing collected packages: smmap, mdurl, tzdata, pygments, markdown-it-py, gitdb, validators, tzlocal, semver, rich, pympler, pydeck, gitpython, blinker, altair, streamlit\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.10.0\n",
      "    Uninstalling Pygments-2.10.0:\n",
      "      Successfully uninstalled Pygments-2.10.0\n",
      "Successfully installed altair-5.2.0 blinker-1.7.0 gitdb-4.0.11 gitpython-3.1.40 markdown-it-py-3.0.0 mdurl-0.1.2 pydeck-0.8.1b0 pygments-2.17.2 pympler-1.0.1 rich-13.7.0 semver-3.0.2 smmap-5.0.1 streamlit-1.12.0 tzdata-2023.3 tzlocal-5.2 validators-0.22.0\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d491225",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jinja2==3.1.2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2==3.1.2) (2.1.1)\n",
      "Installing collected packages: jinja2\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.0.0\n",
      "    Uninstalling Jinja2-3.0.0:\n",
      "      Successfully uninstalled Jinja2-3.0.0\n",
      "Successfully installed jinja2-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n",
      "spacy 3.5.3 requires thinc<8.2.0,>=8.1.8, but you have thinc 8.2.1 which is incompatible.\n",
      "flask-sqlalchemy 3.0.0 requires SQLAlchemy>=1.4.18, but you have sqlalchemy 1.2.19 which is incompatible.\n",
      "cookiecutter 1.7.2 requires Jinja2<3.0.0, but you have jinja2 3.1.2 which is incompatible.\n",
      "cookiecutter 1.7.2 requires MarkupSafe<2.0.0, but you have markupsafe 2.1.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install jinja2==3.1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf004f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
